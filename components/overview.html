<section id="overview" class="section">
    <div class="stats">
        <div class="stats-grid">
            <div class="stat-item">
                <span class="stat-number">102</span>
                <div class="stat-label">Classes</div>
            </div>
            <div class="stat-item">
                <span class="stat-number">80</span>
                <div class="stat-label">Subjects</div>
            </div>
            <div class="stat-item">
                <span class="stat-number">66,232</span>
                <div class="stat-label">Clips</div>
            </div>
            <div class="stat-item">
                <span class="stat-number">80</span>
                <div class="stat-label">Views</div>
            </div>
            <div class="stat-item">
                <span class="stat-number">32</span>
                <div class="stat-label">Joints</div>
            </div>
        </div>
    </div>

    <h2>Key Contributions</h2>
    <div class="contributions">
        <div class="contribution-card">
            <h3>Representation-centric Survey</h3>
            <p>A systematic review that categorizes methods by input representation (Joint, Bone, Motion, Derived) and analyzes their roles in spatial and temporal modeling.</p>
        </div>
        <div class="contribution-card">
            <h3>ANUBIS Dataset</h3>
            <p>A large-scale benchmark with 102 action classes and 80 viewpoints (including systematic rear-view capture), containing complex multi-person, aggressive, and socially relevant actions.</p>
        </div>
        <div class="contribution-card">
            <h3>Comprehensive Benchmarking</h3>
            <p>Evaluation of state-of-the-art models on ANUBIS, revealing representation-dependent performance trends and highlighting the need for task-aware, semantically aligned fusion.</p>
        </div>
    </div>
</section>